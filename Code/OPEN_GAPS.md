# Open Gaps and Missing Information
## Date: January 14, 2026
## Purpose: Track what we DON'T know and need to find out

---

## Critical Gaps (Block Hypothesis Testing)

### GAP-001: Actual Follow-up Conversation Logs
**What's Missing:** Real conversation transcripts showing main query + 3-5 follow-ups

**Why Needed:**
- To confirm H4.1 (context loss) empirically
- To see if filters actually persist or not
- To measure actual vs expected behavior

**Minimum Clarifying Questions:**
1. "Can you provide 3 example conversations with follow-ups?"
2. "For each conversation, show:
   - User query 1 + system answer 1
   - User query 2 (or clicked follow-up) + system answer 2
   - User query 3 + system answer 3"
3. "Include examples where the answer was WRONG"

**How to Fill Gap:**
- Option A: User provides actual conversation logs
- Option B: Run system and collect logs manually
- Option C: Add conversation logging to code

**Impact if Unfilled:**
- Can't confirm H4.1 (85% confidence)
- Can't measure context preservation rate (Metric 1)

---

### GAP-002: Route Decision Logs
**What's Missing:** Actual routing decisions for main query + follow-ups

**Why Needed:**
- To confirm H3.2 (route mismatch)
- To see if follow-ups route to KPI vs RAG correctly

**Minimum Clarifying Questions:**
1. "Add logging to detect_intent() function to print route for each query"
2. "For conversation in GAP-001, what route was taken for each turn?"
3. "Examples of cross-route scenarios (policy question ‚Üí data question)"

**How to Fill Gap:**
```python
# Add to code:
def route_query(query, trace):
    route = detect_intent(query)
    print(f"üîÄ ROUTE: '{query}' ‚Üí {route}")
    trace.route = route
    return route
```

**Impact if Unfilled:**
- Can't confirm H3.2 (75% confidence)
- Can't measure route consistency rate (Metric 2)

---

### GAP-003: Generated Follow-up Question Text
**What's Missing:** Actual follow-up questions generated by `generate_ceo_followup_questions()`

**Why Needed:**
- To confirm H4.3 (vague follow-up generation)
- To see if follow-ups explicitly mention context

**Minimum Clarifying Questions:**
1. "For query 'Show Selangor sales June', what are the 3 generated follow-ups?"
2. "Do follow-ups include 'Selangor' explicitly or just say 'previous month'?"
3. "Provide 10 examples of main query ‚Üí generated follow-ups"

**Example Expected:**
```python
Main: "Show Selangor sales for June 2024"
Generated follow-ups:
1. "Compare with previous month"  # Vague
2. "Show product breakdown"  # Vague
3. "What's driving performance?"  # Vague

Better would be:
1. "Compare Selangor June vs Selangor May"
2. "Show Selangor product breakdown for June 2024"
3. "What drove Selangor's June 2024 sales?"
```

**How to Fill Gap:**
```python
# Log generated follow-ups
followups = generate_ceo_followup_questions(query, answer)
print(f"üìù FOLLOW-UPS for '{query}':")
for i, fq in enumerate(followups, 1):
    print(f"  {i}. {fq}")
```

**Impact if Unfilled:**
- Can't confirm H4.3 (70% confidence)
- Can't design fix for vague follow-up problem

---

### GAP-004: conversation_history Structure
**What's Missing:** Exact structure of conversation_history passed to LLM

**Why Needed:**
- To understand if context CAN be passed (even if it isn't)
- To design fix for H4.1

**Minimum Clarifying Questions:**
1. "What does conversation_history contain? Just text or structured data?"
2. "Example: After 'Show Selangor sales', what's in conversation_history?"
3. "Does it include filter dict {state: 'Selangor'} or just answer text?"

**Expected Structure Options:**
```python
# Option A: Text only (LIMITED)
conversation_history = [
    {"role": "user", "content": "Show Selangor sales June"},
    {"role": "assistant", "content": "Total Sales: RM 16,421.18"}
]

# Option B: With metadata (BETTER)
conversation_history = [
    {
        "role": "user", 
        "content": "Show Selangor sales June",
        "filters": {"state": "Selangor", "month": 202406}
    },
    {
        "role": "assistant", 
        "content": "Total Sales: RM 16,421.18"
    }
]
```

**How to Fill Gap:**
```python
# Add logging before LLM call
print(f"üí¨ conversation_history: {conversation_history}")
```

**Impact if Unfilled:**
- Can't design proper fix for H4.1
- Don't know if filter persistence is possible with current architecture

---

## High Priority Gaps (Affect Reliability)

### GAP-005: Verification Success Rate
**What's Missing:** How often does `verify_answer_against_ground_truth()` catch errors?

**Why Needed:**
- To measure effectiveness of verification layer
- To know if we can trust ‚úÖ checkmarks

**Minimum Clarifying Questions:**
1. "Out of 100 queries, how many get ‚úÖ Verified?"
2. "Has verification ever caught a wrong answer?"
3. "False positive rate? (Says ‚úÖ but answer is wrong)"

**How to Measure:**
```python
# Create test set with ground truth
test_queries = [
    ("Total June sales", 99852.83),
    ("Selangor June sales", 16421.18),
    # ... 98 more
]

correct = 0
false_positive = 0
for query, ground_truth in test_queries:
    answer = system_query(query)
    verified = "‚úÖ Verified" in answer
    actual = extract_number(answer)
    
    if verified and abs(actual - ground_truth) < 5:
        correct += 1
    elif verified and abs(actual - ground_truth) >= 5:
        false_positive += 1

print(f"Verification accuracy: {correct/100:.1%}")
print(f"False positive rate: {false_positive/100:.1%}")
```

**Impact if Unfilled:**
- Don't know if verification actually works
- Can't trust system reliability claims

---

### GAP-006: Hallucination Rate in RAG Answers
**What's Missing:** Quantitative measure of how often LLM invents facts

**Why Needed:**
- To measure H5.1 (hallucination) severity
- To justify or reject "hallucination is a major problem" claim

**Minimum Clarifying Questions:**
1. "Test 20 RAG queries with known negative cases (data not available)"
2. "How many say 'not available' vs invent plausible answer?"
3. "Examples of hallucinated facts?"

**Test Protocol:**
```python
# Negative test cases
negative_cases = [
    "What is the maternity leave policy?",  # Not in docs
    "Show sales for July 2024",  # Data doesn't exist
    "Employee satisfaction score",  # Not measured
    # ... 17 more
]

hallucination_count = 0
for query in negative_cases:
    answer = system_query(query)
    if "not available" in answer.lower() or "don't have" in answer.lower():
        # Good - admitted gap
        pass
    else:
        # Bad - might be hallucination
        print(f"‚ö†Ô∏è Possible hallucination for: {query}")
        print(f"   Answer: {answer[:100]}...")
        hallucination_count += 1

hallucination_rate = hallucination_count / len(negative_cases)
print(f"Hallucination rate: {hallucination_rate:.1%}")
```

**Impact if Unfilled:**
- Don't know if H5.1 is 10% problem or 90% problem
- Can't prioritize hallucination mitigation

---

### GAP-007: Top N Detection Robustness
**What's Missing:** Does "Show top 3" work for all variations?

**Why Needed:**
- Bug #3 was fixed for "top 3" but is it robust?
- Need to test edge cases

**Test Cases:**
```python
test_top_n = [
    ("top 3 products", 3),
    ("Show top 3", 3),
    ("top3 products", 3),
    ("Show me the top 5", 5),
    ("Give me top 7 states", 7),
    ("Best 10 branches", 10),  # "best" instead of "top"
    ("Show 5 best products", 5),
    ("Rank top 3", 3),
]

for query, expected_n in test_top_n:
    answer = system_query(query)
    actual_n = count_items_in_answer(answer)
    status = "‚úÖ" if actual_n == expected_n else "‚ùå"
    print(f"{status} '{query}' ‚Üí Expected {expected_n}, Got {actual_n}")
```

**Impact if Unfilled:**
- Don't know if fix is comprehensive
- Might fail on "best 5" or "rank 7"

---

## Medium Priority Gaps (Improve Understanding)

### GAP-008: Edge Case Coverage
**What's Missing:** Test results for edge cases

**Edge Cases Not Covered in questions.csv:**
1. **Empty result queries**
   - "Sales for July 2024" (doesn't exist)
   - "Employee count in Singapore" (wrong state)
   - Expected: "No data available"
   - Risk: Returns RM 0.00 or crashes

2. **Percentage edge cases**
   - "What % of 0 is Selangor?" (denominator is 0)
   - "What % of May sales is May sales?" (should be 100%)
   - "What % of June sales came from nowhere?" (0%)

3. **Comparison edge cases**
   - "Compare June vs June" (same period)
   - "Compare 2024-06 vs 2024-12" (2024-12 doesn't exist)
   - "Compare with next month" (future period)

4. **Multi-filter complexity**
   - "Top 3 Burger products in Selangor for May 2024 via Delivery channel"
   - 4 filters: Product category, State, Month, Channel

**Minimum Clarifying Questions:**
1. "Test query: 'Sales for July 2024' - What should system say?"
2. "Test query: 'What % of June is June' - Should it say 100%?"
3. "How does system handle non-existent data?"

**How to Fill Gap:**
```python
# Create edge case test suite
edge_cases = [
    ("Sales July 2024", "Should say: Data not available for July 2024"),
    ("What % of June is June", "Should say: 100%"),
    ("Compare June vs June", "Should say: Same period, no change"),
    # ... more edge cases
]

for query, expected_behavior in edge_cases:
    answer = system_query(query)
    print(f"Query: {query}")
    print(f"Expected: {expected_behavior}")
    print(f"Actual: {answer[:100]}...")
    print()
```

**Impact if Unfilled:**
- System might crash or give weird answers on edge cases
- Don't know coverage of tests

---

### GAP-009: Performance Under Load
**What's Missing:** Does system behavior change under concurrent queries?

**Why Needed:**
- FAISS caching, model loading might affect consistency
- Multi-user scenario not tested

**Questions:**
1. "If 2 users query simultaneously, do they get same results?"
2. "Does cache warming affect first vs subsequent queries?"
3. "Response time distribution: p50, p95, p99?"

**Impact if Unfilled:**
- Don't know if production will behave differently
- Can't estimate scale limits

---

### GAP-010: Relative Time Resolution Accuracy
**What's Missing:** Does "this month" always resolve correctly?

**Test Cases:**
```python
# Today is Jan 14, 2026
# Latest data is June 2024

test_relative = [
    ("sales this month", "Should use June 2024, not Jan 2026"),
    ("sales bulan ni", "Should use June 2024"),
    ("sales last month", "Should use May 2024"),
    ("sales bulan lepas", "Should use May 2024"),
    ("sales next month", "Should say: Future data not available"),
]
```

**How to Fill Gap:**
- Test each query, verify month extracted
- Check if system clarifies "latest available" vs "current month"

**Impact if Unfilled:**
- Users might be confused about which month is used
- System might try to find Jan 2026 data (doesn't exist)

---

## Low Priority Gaps (Nice to Have)

### GAP-011: Visual Query Handling
**What's Missing:** No test results for V01-V05 (image queries)

**Why Needed:**
- questions.csv includes 5 visual queries
- No evidence these were tested

**Questions:**
1. "Do V01-V05 queries work?"
2. "OCR accuracy for tables?"
3. "Image file path handling?"

**Impact if Unfilled:**
- Don't know if visual feature works at all
- Might be completely broken

---

### GAP-012: Malay Language Handling
**What's Missing:** Success rate for Malay queries (R01-R06)

**Why Needed:**
- questions.csv has Malay queries
- Need to know if language affects accuracy

**Test:**
```python
malay_queries = [
    ("jumlah jualan bulan jun 2024", "Same as 'total sales June 2024'"),
    ("berapa orang staff", "Same as 'how many employees'"),
]

for malay, english_equivalent in malay_queries:
    answer_malay = system_query(malay)
    answer_english = system_query(english_equivalent)
    # Should extract same numbers
    compare_answers(answer_malay, answer_english)
```

**Impact if Unfilled:**
- Don't know if bilingual feature works
- Malay users might get worse results

---

### GAP-013: Follow-up Click vs Typed Query
**What's Missing:** Do generated follow-ups (clicked) behave differently than typed follow-ups?

**Scenario:**
```
Main: "Show Selangor sales"

Option A: User clicks generated follow-up "Compare with previous month"
Option B: User types "Compare with previous month"

Are results the same or different?
```

**Why It Might Differ:**
- Clicked follow-up might carry metadata (context)
- Typed query has no metadata, only text

**How to Test:**
```python
# Test both paths
main_query = "Selangor sales June"
main_answer = system_query(main_query)

# Path A: Click generated follow-up
followups = generate_followups(main_query, main_answer)
clicked_answer = system_query(followups[0], context=main_answer)

# Path B: Type same text
typed_answer = system_query(followups[0])

# Should they be the same?
if clicked_answer != typed_answer:
    print("‚ö†Ô∏è Clicked vs typed behave differently!")
```

**Impact if Unfilled:**
- Don't know if click mechanism works better
- Can't recommend best UX pattern

---

## Critical Information Needed to Proceed

### Priority 1: Must Have (Blocks Progress)
1. **GAP-001:** Actual conversation logs (3-5 examples)
2. **GAP-002:** Route decision logs (for GAP-001 conversations)
3. **GAP-003:** Generated follow-up text (for GAP-001 conversations)
4. **GAP-004:** conversation_history structure

**Action:** Ask user to provide or enable logging to collect these.

### Priority 2: Should Have (High Value)
1. **GAP-005:** Verification success rate (run test suite)
2. **GAP-006:** Hallucination rate (test negative cases)
3. **GAP-007:** Top N robustness (test variations)

**Action:** Create test scripts and run locally.

### Priority 3: Nice to Have (Improves Coverage)
1. **GAP-008:** Edge case coverage
2. **GAP-010:** Relative time resolution
3. **GAP-012:** Malay language handling

**Action:** Expand test suite over time.

---

## Questions for User

### Immediate Questions (To Fill Critical Gaps)
1. **Can you run the system and collect logs for 3 example conversations?**
   - Each should have: Main query ‚Üí 2-3 follow-ups
   - Include one where answer was WRONG
   
2. **Can you add logging to print:**
   - Route taken for each query?
   - Generated follow-up questions?
   - conversation_history structure before LLM call?

3. **Which gaps should I prioritize?**
   - What information can you provide easily?
   - What requires code changes?

### Testing Questions (To Measure System)
1. **Have you tested negative cases?** (D15-type: data not available)
   - How often does system admit "I don't have that data"?
   
2. **Have you tested edge cases?** (July 2024, 100%, 0%)
   - What happens with non-existent data?
   
3. **Do visual queries (V01-V05) work?**
   - Have you tested image input?

### Design Questions (To Understand Architecture)
1. **How is conversation_history currently structured?**
   - Text only or with metadata?
   
2. **Is there a mechanism to pass filters between turns?**
   - Intentional design or missing feature?
   
3. **How are clicked follow-ups different from typed queries?**
   - Do they carry extra context?

---

## Next Steps Based on Gap Status

### If GAP-001 to GAP-004 Filled:
‚Üí Can confirm H4.1 (context loss) empirically
‚Üí Can design fix for filter persistence
‚Üí Can test fix effectiveness

### If GAP-005 to GAP-007 Filled:
‚Üí Can measure system reliability quantitatively
‚Üí Can prioritize improvements based on data
‚Üí Can report confidence levels with evidence

### If All Gaps Filled:
‚Üí Can provide comprehensive system assessment
‚Üí Can recommend fixes with confidence
‚Üí Can predict failure modes accurately

---

## Self-Critique: What Gaps Am I Missing?

### Potential Blind Spots
1. **User expectations** - What do users ACTUALLY want vs what system provides?
2. **Real-world query distribution** - What queries are most common in practice?
3. **Error recovery** - When system says "error", can user recover?
4. **Trust indicators** - How do users know when to trust the answer?

### Meta-Gaps (Gaps about Gaps)
1. Don't know what I don't know about the codebase
2. Haven't seen actual user complaints (if any)
3. Don't know deployment environment constraints
4. Don't know business priorities (speed vs accuracy vs cost)

---

## STATUS: NEEDS INPUT

**Critical Gaps Identified:** 13 total
- Critical (blocks testing): 4
- High priority: 3
- Medium priority: 3
- Low priority: 3

**To Proceed, Need:**
1. Conversation logs (GAP-001)
2. Route logs (GAP-002)
3. Follow-up text (GAP-003)
4. History structure (GAP-004)

**Once Filled:**
- Can confirm remaining hypotheses
- Can design targeted fixes
- Can measure system reliability

**User Action Required:**
- Provide conversation examples OR
- Enable logging and run system OR
- Answer design questions about architecture
