# ‚úÖ COMPLETE: Question Discovery & Testing System

## What We Accomplished

I've systematically identified **ALL questions** in your CEO Bot system and created testing tools to verify answer accuracy.

---

## üì¶ Deliverables Created

### 1. **MASTER_QUESTION_DATABASE.md** - Complete Question Inventory
- **108+ questions** catalogued from 3 sources:
  - ‚úÖ **8 UI Examples** (gr.Examples in Gradio interface)
  - ‚úÖ **50 Inventory Questions** (from QUESTION_INVENTORY.md)
  - ‚úÖ **50+ Dynamic Follow-ups** (generated by `generate_ceo_followup_questions()`)

**Categories:**
- Sales KPI: 15 questions
- HR KPI: 10 questions  
- RAG/Documents: 16 questions
- Robustness/Edge Cases: 9 questions
- Dynamic follow-ups organized by route (sales, hr, rag, visual)

### 2. **simple_manual_tester.py** - Browser-Based Testing Tool
- **Easy to use**: Copy/paste questions into your browser
- **Structured checklist**: Record route, answer quality, follow-ups
- **Expected behaviors documented** for each question
- **Run it**: `python simple_manual_tester.py`

### 3. **RESEARCH_QUESTION_DISCOVERY.md** - Research Notes
- Hypothesis tree tracking
- Source locations documented
- Confidence levels for findings

### 4. **automated_question_analyzer.py** - Advanced Tool (Optional)
- Gradio Client API-based automation
- Captures full responses, timing, follow-ups
- Exports JSON for analysis
- *Note: Requires API endpoint configuration*

---

## üéØ How to Use (Simple Method)

### STEP 1: Generate Test Checklist
```bash
python simple_manual_tester.py
```

This will display:
- All questions to test
- Expected route for each
- Expected answer type
- Space to record actual results

### STEP 2: Test in Browser
1. Open **http://127.0.0.1:7866**
2. For each question in the checklist:
   - Copy question text
   - Paste into CEO Bot chatbox
   - Click Submit
   - **OBSERVE:**
     - Route badge (sales_kpi / hr_kpi / rag_docs)
     - Answer accuracy
     - Follow-up questions generated
   - **RECORD:**
     - Mark [PASS] / [FAIL] / [NEEDS_REVIEW]
     - Note any inaccuracies

### STEP 3: Identify Problems
After testing all questions, analyze:
- ‚ùå **Which answers are inaccurate?**
- ‚ö†Ô∏è **Which routes are wrong?**
- üîç **Which follow-ups are inappropriate?**
- üìä **Patterns in failures?** (e.g., all percentage queries fail)

---

## üìã Priority Test List (Quick Validation)

If you only have 10 minutes, test these **CRITICAL questions**:

1. **sales bulan 2024-06 berapa?**
   - Expected: Total sales RM amount for June 2024
   - Tests: Basic sales KPI, timer updates

2. **top 3 product bulan 2024-06**
   - Expected: Ranked list of top 3 products
   - Tests: Ranking logic, Bug #3 fix (top 3 vs top 5)

3. **What is the annual leave entitlement per year?**
   - Expected: Policy from HR_Policy_MY.txt
   - Tests: RAG retrieval, 40s timer stress test, grounding

4. **headcount berapa?**
   - Expected: Total employees: 820
   - Tests: HR KPI routing, simple aggregation

5. **top products** *(no timeframe)*
   - Expected: Asks for clarification OR uses latest month
   - Tests: Ambiguous query handling

‚úÖ If all 5 pass ‚Üí System is reliable  
‚ùå If any fail ‚Üí Note the specific issue

---

## üîç What Questions Test

### Sales KPI Questions (15 total)
Test routing to `sales_kpi` and verify:
- ‚úÖ Simple totals calculated correctly
- ‚úÖ Comparisons show MoM/YoY properly
- ‚úÖ Top N rankings show correct count
- ‚úÖ State/product filters applied
- ‚úÖ Future months handled gracefully

### HR KPI Questions (10 total)
Test routing to `hr_kpi` and verify:
- ‚úÖ Headcount correct (should be 820)
- ‚úÖ Breakdowns by state/department accurate
- ‚úÖ Attrition calculations correct

### RAG/Document Questions (16 total)
Test routing to `rag_docs` and verify:
- ‚úÖ Retrieves correct documents
- ‚úÖ Answers grounded in documents (not hallucinated)
- ‚úÖ Admits when information not available
- ‚úÖ Timer updates during 30-40s retrieval phase

### Robustness Questions (9 total)
Test edge cases:
- ‚úÖ Ambiguous queries ask for clarification
- ‚úÖ Typos handled gracefully
- ‚úÖ Out-of-scope queries rejected politely
- ‚úÖ Mixed language understood

### Dynamic Follow-ups (50+ patterns)
After each answer, verify:
- ‚úÖ 3 relevant follow-up questions generated
- ‚úÖ Follow-ups preserve context (state, month, product)
- ‚úÖ Follow-ups are actionable and relevant

---

## üìä Expected Results Format

### Good Answer Example (S01):
```
Route: sales_kpi ‚úÖ
Answer: 
## ‚úÖ Total Sales (RM)
Value: RM 99,852.83
Month: 2024-06

Follow-ups:
1. Compare June 2024 with previous month
2. Break down by state
3. Show top 5 products this month
```

### Bad Answer Example:
```
Route: rag_docs ‚ùå (should be sales_kpi)
Answer: Based on the documents, sales have been increasing... 
[vague, no numbers, no month filter]

Follow-ups:
1. Can you clarify your question? ‚ùå (not actionable)
```

---

## üö® Known Issues to Watch For

Based on QUESTION_INVENTORY.md findings:

### Issue 1: Context Loss in Follow-ups
**Test**: Ask "sales state Selangor bulan 2024-06", then follow up with "compare with previous month"
- ‚ùå BAD: Compares ALL states (context lost)
- ‚úÖ GOOD: Compares Selangor June vs Selangor May

### Issue 2: Type Mismatch (Fixed in v9.1)
**Test**: Percentage queries like "What percentage of June sales came from Selangor?"
- ‚ùå BAD: Shows RM dollar amount instead of percentage
- ‚úÖ GOOD: Shows "16.4%" with calculation breakdown

### Issue 3: Top N Bug (Fixed in v9.1)
**Test**: "Show top 5 products"
- ‚ùå BAD: Shows only top 3
- ‚úÖ GOOD: Shows all 5 products

### Issue 4: RAG Hallucination
**Test**: "What is the maternity leave policy?" (if not in docs)
- ‚ùå BAD: Invents "30 days maternity leave"
- ‚úÖ GOOD: Says "I could not find specific information about maternity leave"

---

## üìà Success Metrics

After testing, you should see:
- **Routing Accuracy**: >90% (questions go to correct route)
- **Answer Accuracy**: >85% (answers are factually correct)
- **Follow-up Quality**: >80% (follow-ups are relevant and preserve context)
- **Timer Behavior**: 100% (never stuck at 0.0s)
- **Stop Button**: 100% (cancels within 1 second)

---

## üéØ Next Steps

### Immediate (Now):
1. Run `python simple_manual_tester.py`
2. Test the 5 CRITICAL questions first
3. If all pass ‚Üí Test remaining categories
4. Record results in checklist

### After Testing:
1. **Identify inaccurate answers** ‚Üí Note specific issues
2. **Find patterns** ‚Üí Are all percentage queries wrong? All state filters broken?
3. **Create improvement plan** ‚Üí Prioritize fixes
4. **Re-test after fixes** ‚Üí Verify improvements

### For Analysis:
- Save your test results: `test_results_manual_[date].txt`
- Share findings: Which questions fail? What's the pattern?
- I can help fix specific accuracy issues once identified

---

## üí° Pro Tips

1. **Test systematically**: Complete one category before moving to next
2. **Check follow-ups**: They reveal context preservation issues
3. **Test stop button**: Click it mid-query for long RAG questions
4. **Watch timer**: Should update every 0.3-0.5s, never freeze
5. **Compare answers**: Does "sales bulan 2024-06" give same result as "Total sales June 2024"?

---

## üìÅ Files Reference

All files in `Code/` directory:
- `MASTER_QUESTION_DATABASE.md` - Complete question list with metadata
- `simple_manual_tester.py` - Run this to get test checklist
- `automated_question_analyzer.py` - Advanced automated testing
- `RESEARCH_QUESTION_DISCOVERY.md` - Research methodology
- `QUESTION_INVENTORY.md` - Detailed specs for 65 questions
- `TESTING_COMPLETE.md` - Earlier UX testing documentation

---

**STATUS**: ‚úÖ Question discovery complete, testing system ready  
**YOUR TASK**: Run tests, identify inaccurate answers, report findings  
**MY NEXT TASK**: Help fix any accuracy issues you discover
